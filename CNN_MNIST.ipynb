{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 70_000\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCH =20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset, mnist_info = tfds.load(name=\"mnist\", with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the train and test datasets\n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a function to scale the value[0,255] to value[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(image, label):\n",
    "    image =tf.cast(image, tf.float32) #to avoid problems with data types\n",
    "    image /= 255.\n",
    "\n",
    "    return image, label #need to include label in here because the map function below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apply above function to both mnist_train and mnist_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_validation_data = mnist_train.map(scale)\n",
    "test_data = mnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "obtain the number training/validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_validation_samples =0.1 * mnist_info.splits[\"train\"].num_examples\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "same with test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_samples = mnist_info.splits[\"test\"].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_validation_data = train_and_validation_data.shuffle(BUFFER_SIZE) #in this case MNIST dataset is relatively small we can shuffle in one go \n",
    "                                                                           #but for larger datasets we can set the BUFFER_SIZE to shuffle in batches\n",
    "                                                                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split our training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_and_validation_data.skip(num_validation_samples) #return everything except the first 10%\n",
    "validation_data = train_and_validation_data.take(num_validation_samples) #return the first 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch our dataset to improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.batch(batch_size=BATCH_SIZE) #larger batch size improve performance especially on GPU \n",
    "                                                     #but smaller batch size may provide better test accuracy and it's desirable to set it as power of 2\n",
    "#validation and test sets don't need to be batched as we don't backward propagate on them however the model expects it to be batched to get the proper dimensions\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "test_data = test_data.batch(num_test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model and training it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(50, 5, activation=\"relu\", input_shape=(28 , 28, 1)), #first layer with 50 kernel and size of 5 activation function of choice is relu\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2,2)), #second layer, pool_size(2,2) is the default but we specified it for clarity\n",
    "    tf.keras.layers.Conv2D(50, 3, activation=\"relu\"), #input_shape removed as it's only needed in the first layer and smaller kernel size to accommandate smaller images\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "    tf.keras.layers.Flatten(),  #this layer's purpose is to turn a multi-dimensional tensor into a a one-dimensional vector so we can perform classification in the next layer\n",
    "    tf.keras.layers.Dense(10) #it's impossible to produce a numerically stable loss function for all models so we won't define activation function here\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "___________________________________________________________________________\n",
      " Layer (type)                    Output Shape                  Param #     \n",
      "===========================================================================\n",
      " conv2d_4 (Conv2D)               (None, 24, 24, 50)            1300        \n",
      "                                                                           \n",
      " max_pooling2d_4 (MaxPooling2D)  (None, 12, 12, 50)            0           \n",
      "                                                                           \n",
      " conv2d_5 (Conv2D)               (None, 10, 10, 50)            22550       \n",
      "                                                                           \n",
      " max_pooling2d_5 (MaxPooling2D)  (None, 5, 5, 50)              0           \n",
      "                                                                           \n",
      " flatten_2 (Flatten)             (None, 1250)                  0           \n",
      "                                                                           \n",
      " dense_2 (Dense)                 (None, 10)                    12510       \n",
      "                                                                           \n",
      "===========================================================================\n",
      "Total params: 36,360\n",
      "Trainable params: 36,360\n",
      "Non-trainable params: 0\n",
      "___________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary(line_length=75) #this is to check if our model initialised correctly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) #from_logits = True tells tf to use softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=loss_fn, metrics=[\"accuracy\"]) # this combines our model with loss function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = \"val_loss\", #monitor validation loss during training\n",
    "    mode = \"auto\",\n",
    "    min_delta = 0,\n",
    "    patience = 2,\n",
    "    verbose = 0,\n",
    "    restore_best_weights = True #this is set so that if validation loss increases for two epoch programme will stop\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"log\\\\fit\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "422/422 [==============================] - 3s 4ms/step - loss: 0.2716 - accuracy: 0.9233 - val_loss: 0.0782 - val_accuracy: 0.9760\n",
      "Epoch 2/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0707 - accuracy: 0.9785 - val_loss: 0.0509 - val_accuracy: 0.9850\n",
      "Epoch 3/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0525 - accuracy: 0.9844 - val_loss: 0.0415 - val_accuracy: 0.9872\n",
      "Epoch 4/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0429 - accuracy: 0.9867 - val_loss: 0.0371 - val_accuracy: 0.9888\n",
      "Epoch 5/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0366 - accuracy: 0.9887 - val_loss: 0.0341 - val_accuracy: 0.9907\n",
      "Epoch 6/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0308 - accuracy: 0.9902 - val_loss: 0.0229 - val_accuracy: 0.9928\n",
      "Epoch 7/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0278 - accuracy: 0.9915 - val_loss: 0.0248 - val_accuracy: 0.9922\n",
      "Epoch 8/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0235 - accuracy: 0.9931 - val_loss: 0.0207 - val_accuracy: 0.9933\n",
      "Epoch 9/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0224 - accuracy: 0.9930 - val_loss: 0.0127 - val_accuracy: 0.9962\n",
      "Epoch 10/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0192 - accuracy: 0.9939 - val_loss: 0.0192 - val_accuracy: 0.9947\n",
      "Epoch 11/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0169 - accuracy: 0.9951 - val_loss: 0.0124 - val_accuracy: 0.9958\n",
      "Epoch 12/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0155 - accuracy: 0.9948 - val_loss: 0.0198 - val_accuracy: 0.9917\n",
      "Epoch 13/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0136 - accuracy: 0.9959 - val_loss: 0.0114 - val_accuracy: 0.9962\n",
      "Epoch 14/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0112 - accuracy: 0.9966 - val_loss: 0.0056 - val_accuracy: 0.9990\n",
      "Epoch 15/20\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 0.0102 - accuracy: 0.9969 - val_loss: 0.0092 - val_accuracy: 0.9972\n",
      "Epoch 16/20\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 0.0094 - accuracy: 0.9971 - val_loss: 0.0098 - val_accuracy: 0.9972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22d6f6d6560>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_data,\n",
    "    epochs = NUM_EPOCH,\n",
    "    callbacks = [tensorboard_callback, early_stopping],  #it's possible to use more than one callbacks hence the []\n",
    "                                                         #note early_stopping should be the last element in the list\n",
    "                                                         #otherwise model may bug out\n",
    "    validation_data = validation_data,\n",
    "    verbose = 1 #2 means info will only be printed at end of each epoch\n",
    "                #verbose = 2 seems to introduce an error in tensorboard\n",
    "                #changed to 1 for this reason\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the training stopped at 15th Epoch due to val_loss increase for 2 Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 153ms/step - loss: 0.0328 - accuracy: 0.9907\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.0328. Test accuracy:  99.07%\n"
     ]
    }
   ],
   "source": [
    "print(\"Test loss: {0: .4f}. Test accuracy: {1: .2f}%\".format(test_loss, test_accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over 99% Accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting images and the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the test data into 2 arrays, containing the images and the corresponding labels\n",
    "for images, labels in test_data.take(1):\n",
    "    images_test = images.numpy()\n",
    "    labels_test = labels.numpy()\n",
    "\n",
    "#reshape the image into 28x28 form, suitable for matplotlib (original dimension 28x28x1)\n",
    "images_plot = np.reshape(images_test, (10000,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 2\n"
     ]
    }
   ],
   "source": [
    "# The image to be displayed and tested\n",
    "i = 1\n",
    "\n",
    "\n",
    "# Plot the image\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.axis('off')\n",
    "plt.imshow(images_plot[i-1], cmap=\"gray\", aspect='auto')\n",
    "plt.show()\n",
    "\n",
    "# Print the correct label for the image\n",
    "print(\"Label: {}\".format(labels_test[i-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 10 artists>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Obtain the model's predictions (logits)\n",
    "predictions = model.predict(images_test[i-1:i])\n",
    "\n",
    "# Convert those predictions into probabilities (recall that we incorporated the softmaxt activation into the loss function)\n",
    "probabilities = tf.nn.softmax(predictions).numpy()\n",
    "# Convert the probabilities into percentages\n",
    "probabilities = probabilities*100\n",
    "\n",
    "\n",
    "# Create a bar chart to plot the probabilities for each class\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.bar(x=[1,2,3,4,5,6,7,8,9,10], height=probabilities[0], tick_label=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above graph shows our model have almost 100% confidence in the number 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 7748), started 3:04:17 ago. (Use '!kill 7748' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c38897849635f0d7\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c38897849635f0d7\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir \"logs/fit\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If tensorboard doesn't load try the following commands in cmd:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taskkill /im tensorboard.exe /f\n",
    "# del /q %TMP%\\.tensorboard-info\\*\n",
    "\n",
    "#this will end existing tensorboard process and clean any temp data associated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflowgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
