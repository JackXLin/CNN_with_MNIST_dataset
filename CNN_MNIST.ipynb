{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorboard.plugins.hparams import api as hp #used for hyperparameter tuning\n",
    "import datetime\n",
    "\n",
    "import io\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 70_000\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCH =20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset, mnist_info = tfds.load(name=\"mnist\", with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the train and test datasets\n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a function to scale the value[0,255] to value[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(image, label):\n",
    "    image =tf.cast(image, tf.float32) #to avoid problems with data types\n",
    "    image /= 255.\n",
    "\n",
    "    return image, label #need to include label in here because the map function below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apply above function to both mnist_train and mnist_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_validation_data = mnist_train.map(scale)\n",
    "test_data = mnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "obtain the number training/validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_validation_samples =0.1 * mnist_info.splits[\"train\"].num_examples\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "same with test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_samples = mnist_info.splits[\"test\"].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_validation_data = train_and_validation_data.shuffle(BUFFER_SIZE) #in this case MNIST dataset is relatively small we can shuffle in one go \n",
    "                                                                           #but for larger datasets we can set the BUFFER_SIZE to shuffle in batches\n",
    "                                                                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split our training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_and_validation_data.skip(num_validation_samples) #return everything except the first 10%\n",
    "validation_data = train_and_validation_data.take(num_validation_samples) #return the first 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch our dataset to improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.batch(batch_size=BATCH_SIZE) #larger batch size improve performance especially on GPU \n",
    "                                                     #but smaller batch size may provide better test accuracy and it's desirable to set it as power of 2\n",
    "#validation and test sets don't need to be batched as we don't backward propagate on them however the model expects it to be batched to get the proper dimensions\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "test_data = test_data.batch(num_test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract data from validation and convert them into numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in validation_data:\n",
    "    images_val = images.numpy()\n",
    "    labels_val = labels.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_FILTER_SIZE = hp.HParam('filter_size', hp.Discrete([3, 5, 7])) #kernel size 3x3 5x5 7x7\n",
    "HP_OPTIMIZER = hp.HParam(\"optimizer\", hp.Discrete([\"adam\", \"sgd\"])) #optimizer adam and stochastic gradient descent\n",
    "                                                                     #adam is much sophisticated than sgd so we expect it will\n",
    "                                                                     #outperform sgd\n",
    "METRIC_ACCURACY = \"accuracy\"\n",
    "\n",
    "with tf.summary.create_file_writer(\"logs/hparam_tuning\").as_default():\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_FILTER_SIZE, HP_OPTIMIZER],\n",
    "        metrics=[hp.Metric(METRIC_ACCURACY, display_name=\"Accuracy\")]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cerating functions for training our model and for logging purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(hparams):\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(50, hparams[HP_FILTER_SIZE], activation=\"relu\", input_shape=(28 , 28, 1)), #replaced kernel size with hparams\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2,2)), \n",
    "    tf.keras.layers.Conv2D(50, hparams[HP_FILTER_SIZE], activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "    tf.keras.layers.Flatten(),  #this layer's purpose is to turn a multi-dimensional tensor into a a one-dimensional vector so we can perform classification in the next layer\n",
    "    tf.keras.layers.Dense(10) #it's impossible to produce a numerically stable loss function for all models so we won't define activation function here\n",
    "])\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n",
    "    model.compile(optimizer=hparams[HP_OPTIMIZER], loss=loss_fn, metrics=[\"accuracy\"]) #replaced optimizer with hparams\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = \"val_loss\", #monitor validation loss during training\n",
    "    mode = \"auto\",\n",
    "    min_delta = 0,\n",
    "    patience = 2,\n",
    "    verbose = 0,\n",
    "    restore_best_weights = True #this is set so that if validation loss increases for two epoch programme will stop\n",
    ")\n",
    "    model.fit(\n",
    "    train_data,\n",
    "    epochs = NUM_EPOCH,\n",
    "    callbacks = [early_stopping],  \n",
    "    validation_data = validation_data,\n",
    "    verbose = 1 \n",
    ")\n",
    "    _, accuracy = model.evaluate(test_data)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(log_dir, hparams):\n",
    "\n",
    "    with tf.summary.create_file_writer(log_dir).as_default():\n",
    "        hp.hparams(hparams) #record the values used in this trial\n",
    "        accuracy = train_test_model(hparams)\n",
    "        tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model with different hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-0\n",
      "{'filter_size': 3, 'optimizer': 'adam'}\n",
      "Epoch 1/20\n",
      "422/422 [==============================] - 8s 5ms/step - loss: 0.2986 - accuracy: 0.9162 - val_loss: 0.0935 - val_accuracy: 0.9712\n",
      "Epoch 2/20\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 0.0796 - accuracy: 0.9755 - val_loss: 0.0583 - val_accuracy: 0.9805\n",
      "Epoch 3/20\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 0.0582 - accuracy: 0.9824 - val_loss: 0.0430 - val_accuracy: 0.9873\n",
      "Epoch 4/20\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 0.0470 - accuracy: 0.9855 - val_loss: 0.0396 - val_accuracy: 0.9872\n",
      "Epoch 5/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0412 - accuracy: 0.9875 - val_loss: 0.0447 - val_accuracy: 0.9865\n",
      "Epoch 6/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0354 - accuracy: 0.9890 - val_loss: 0.0286 - val_accuracy: 0.9917\n",
      "Epoch 7/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0308 - accuracy: 0.9903 - val_loss: 0.0191 - val_accuracy: 0.9937\n",
      "Epoch 8/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0284 - accuracy: 0.9909 - val_loss: 0.0220 - val_accuracy: 0.9935\n",
      "Epoch 9/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0245 - accuracy: 0.9926 - val_loss: 0.0216 - val_accuracy: 0.9945\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 0.0356 - accuracy: 0.9885\n",
      "--- Starting trial: run-1\n",
      "{'filter_size': 3, 'optimizer': 'sgd'}\n",
      "Epoch 1/20\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.4935 - accuracy: 0.5948 - val_loss: 0.5140 - val_accuracy: 0.8588\n",
      "Epoch 2/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.3989 - accuracy: 0.8846 - val_loss: 0.3381 - val_accuracy: 0.8985\n",
      "Epoch 3/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.3025 - accuracy: 0.9111 - val_loss: 0.2719 - val_accuracy: 0.9190\n",
      "Epoch 4/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.2486 - accuracy: 0.9271 - val_loss: 0.2273 - val_accuracy: 0.9303\n",
      "Epoch 5/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.2166 - accuracy: 0.9373 - val_loss: 0.1993 - val_accuracy: 0.9410\n",
      "Epoch 6/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.1895 - accuracy: 0.9448 - val_loss: 0.1740 - val_accuracy: 0.9482\n",
      "Epoch 7/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.1677 - accuracy: 0.9508 - val_loss: 0.1581 - val_accuracy: 0.9532\n",
      "Epoch 8/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.1518 - accuracy: 0.9556 - val_loss: 0.1392 - val_accuracy: 0.9618\n",
      "Epoch 9/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.1395 - accuracy: 0.9596 - val_loss: 0.1269 - val_accuracy: 0.9633\n",
      "Epoch 10/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.1291 - accuracy: 0.9624 - val_loss: 0.1272 - val_accuracy: 0.9637\n",
      "Epoch 11/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.1195 - accuracy: 0.9654 - val_loss: 0.1079 - val_accuracy: 0.9678\n",
      "Epoch 12/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.1145 - accuracy: 0.9666 - val_loss: 0.1099 - val_accuracy: 0.9663\n",
      "Epoch 13/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.1079 - accuracy: 0.9687 - val_loss: 0.1041 - val_accuracy: 0.9703\n",
      "Epoch 14/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.1047 - accuracy: 0.9689 - val_loss: 0.1035 - val_accuracy: 0.9693\n",
      "Epoch 15/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0981 - accuracy: 0.9708 - val_loss: 0.1040 - val_accuracy: 0.9703\n",
      "Epoch 16/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0950 - accuracy: 0.9713 - val_loss: 0.1000 - val_accuracy: 0.9707\n",
      "Epoch 17/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0917 - accuracy: 0.9726 - val_loss: 0.0879 - val_accuracy: 0.9738\n",
      "Epoch 18/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0892 - accuracy: 0.9737 - val_loss: 0.0877 - val_accuracy: 0.9748\n",
      "Epoch 19/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0854 - accuracy: 0.9742 - val_loss: 0.0889 - val_accuracy: 0.9742\n",
      "Epoch 20/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0815 - accuracy: 0.9759 - val_loss: 0.0735 - val_accuracy: 0.9788\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0713 - accuracy: 0.9789\n",
      "--- Starting trial: run-2\n",
      "{'filter_size': 5, 'optimizer': 'adam'}\n",
      "Epoch 1/20\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 0.2482 - accuracy: 0.9285 - val_loss: 0.0880 - val_accuracy: 0.9740\n",
      "Epoch 2/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0682 - accuracy: 0.9800 - val_loss: 0.0435 - val_accuracy: 0.9870\n",
      "Epoch 3/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0467 - accuracy: 0.9856 - val_loss: 0.0363 - val_accuracy: 0.9885\n",
      "Epoch 4/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0372 - accuracy: 0.9884 - val_loss: 0.0318 - val_accuracy: 0.9900\n",
      "Epoch 5/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0320 - accuracy: 0.9904 - val_loss: 0.0237 - val_accuracy: 0.9938\n",
      "Epoch 6/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0250 - accuracy: 0.9925 - val_loss: 0.0330 - val_accuracy: 0.9897\n",
      "Epoch 7/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0231 - accuracy: 0.9926 - val_loss: 0.0199 - val_accuracy: 0.9940\n",
      "Epoch 8/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0189 - accuracy: 0.9939 - val_loss: 0.0118 - val_accuracy: 0.9965\n",
      "Epoch 9/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0156 - accuracy: 0.9949 - val_loss: 0.0118 - val_accuracy: 0.9960\n",
      "Epoch 10/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0139 - accuracy: 0.9956 - val_loss: 0.0101 - val_accuracy: 0.9968\n",
      "Epoch 11/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0118 - accuracy: 0.9962 - val_loss: 0.0092 - val_accuracy: 0.9972\n",
      "Epoch 12/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0116 - accuracy: 0.9966 - val_loss: 0.0062 - val_accuracy: 0.9983\n",
      "Epoch 13/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0094 - accuracy: 0.9968 - val_loss: 0.0066 - val_accuracy: 0.9987\n",
      "Epoch 14/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0090 - accuracy: 0.9972 - val_loss: 0.0063 - val_accuracy: 0.9978\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 0.0284 - accuracy: 0.9908\n",
      "--- Starting trial: run-3\n",
      "{'filter_size': 5, 'optimizer': 'sgd'}\n",
      "Epoch 1/20\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.1697 - accuracy: 0.7081 - val_loss: 0.4010 - val_accuracy: 0.8870\n",
      "Epoch 2/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.3200 - accuracy: 0.9076 - val_loss: 0.2646 - val_accuracy: 0.9240\n",
      "Epoch 3/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.2355 - accuracy: 0.9306 - val_loss: 0.2037 - val_accuracy: 0.9368\n",
      "Epoch 4/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.1908 - accuracy: 0.9434 - val_loss: 0.1757 - val_accuracy: 0.9460\n",
      "Epoch 5/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.1633 - accuracy: 0.9526 - val_loss: 0.1494 - val_accuracy: 0.9590\n",
      "Epoch 6/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.1425 - accuracy: 0.9581 - val_loss: 0.1180 - val_accuracy: 0.9650\n",
      "Epoch 7/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.1271 - accuracy: 0.9633 - val_loss: 0.1219 - val_accuracy: 0.9648\n",
      "Epoch 8/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.1164 - accuracy: 0.9659 - val_loss: 0.1125 - val_accuracy: 0.9683\n",
      "Epoch 9/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.1067 - accuracy: 0.9688 - val_loss: 0.1096 - val_accuracy: 0.9678\n",
      "Epoch 10/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.1008 - accuracy: 0.9701 - val_loss: 0.0953 - val_accuracy: 0.9732\n",
      "Epoch 11/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0935 - accuracy: 0.9728 - val_loss: 0.0870 - val_accuracy: 0.9737\n",
      "Epoch 12/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0886 - accuracy: 0.9740 - val_loss: 0.0888 - val_accuracy: 0.9748\n",
      "Epoch 13/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0844 - accuracy: 0.9751 - val_loss: 0.0764 - val_accuracy: 0.9785\n",
      "Epoch 14/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0807 - accuracy: 0.9759 - val_loss: 0.0754 - val_accuracy: 0.9773\n",
      "Epoch 15/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0775 - accuracy: 0.9764 - val_loss: 0.0702 - val_accuracy: 0.9785\n",
      "Epoch 16/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0730 - accuracy: 0.9780 - val_loss: 0.0759 - val_accuracy: 0.9772\n",
      "Epoch 17/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0721 - accuracy: 0.9792 - val_loss: 0.0725 - val_accuracy: 0.9785\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0654 - accuracy: 0.9806\n",
      "--- Starting trial: run-4\n",
      "{'filter_size': 7, 'optimizer': 'adam'}\n",
      "Epoch 1/20\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 0.2406 - accuracy: 0.9304 - val_loss: 0.0815 - val_accuracy: 0.9775\n",
      "Epoch 2/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0697 - accuracy: 0.9788 - val_loss: 0.0436 - val_accuracy: 0.9855\n",
      "Epoch 3/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0500 - accuracy: 0.9846 - val_loss: 0.0391 - val_accuracy: 0.9868\n",
      "Epoch 4/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0386 - accuracy: 0.9883 - val_loss: 0.0363 - val_accuracy: 0.9883\n",
      "Epoch 5/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0333 - accuracy: 0.9896 - val_loss: 0.0308 - val_accuracy: 0.9918\n",
      "Epoch 6/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0266 - accuracy: 0.9917 - val_loss: 0.0205 - val_accuracy: 0.9935\n",
      "Epoch 7/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0230 - accuracy: 0.9924 - val_loss: 0.0124 - val_accuracy: 0.9963\n",
      "Epoch 8/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0188 - accuracy: 0.9939 - val_loss: 0.0155 - val_accuracy: 0.9948\n",
      "Epoch 9/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0161 - accuracy: 0.9947 - val_loss: 0.0128 - val_accuracy: 0.9955\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 0.0299 - accuracy: 0.9907\n",
      "--- Starting trial: run-5\n",
      "{'filter_size': 7, 'optimizer': 'sgd'}\n",
      "Epoch 1/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 1.0097 - accuracy: 0.7543 - val_loss: 0.4017 - val_accuracy: 0.8938\n",
      "Epoch 2/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.3256 - accuracy: 0.9079 - val_loss: 0.2682 - val_accuracy: 0.9240\n",
      "Epoch 3/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.2449 - accuracy: 0.9296 - val_loss: 0.2363 - val_accuracy: 0.9318\n",
      "Epoch 4/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.1998 - accuracy: 0.9424 - val_loss: 0.1891 - val_accuracy: 0.9452\n",
      "Epoch 5/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.1712 - accuracy: 0.9495 - val_loss: 0.1642 - val_accuracy: 0.9498\n",
      "Epoch 6/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.1517 - accuracy: 0.9553 - val_loss: 0.1481 - val_accuracy: 0.9557\n",
      "Epoch 7/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.1357 - accuracy: 0.9604 - val_loss: 0.1346 - val_accuracy: 0.9620\n",
      "Epoch 8/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.1238 - accuracy: 0.9639 - val_loss: 0.1225 - val_accuracy: 0.9648\n",
      "Epoch 9/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.1157 - accuracy: 0.9665 - val_loss: 0.1149 - val_accuracy: 0.9655\n",
      "Epoch 10/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.1075 - accuracy: 0.9693 - val_loss: 0.0960 - val_accuracy: 0.9720\n",
      "Epoch 11/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.1012 - accuracy: 0.9706 - val_loss: 0.1032 - val_accuracy: 0.9703\n",
      "Epoch 12/20\n",
      "422/422 [==============================] - 2s 3ms/step - loss: 0.0951 - accuracy: 0.9725 - val_loss: 0.0967 - val_accuracy: 0.9722\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0998 - accuracy: 0.9717\n"
     ]
    }
   ],
   "source": [
    "session_num = 0\n",
    "\n",
    "for filter_size in HP_FILTER_SIZE.domain.values: #nested for loops to go through all combination of hyperparameters\n",
    "    for optimizer in HP_OPTIMIZER.domain.values:\n",
    "\n",
    "        hparams = {                            #current values for hyperparameter\n",
    "            HP_FILTER_SIZE:filter_size,\n",
    "            HP_OPTIMIZER: optimizer\n",
    "        }\n",
    "        run_name = \"run-%d\" % session_num\n",
    "        print(\"--- Starting trial: %s\" %run_name)     #print out useful info for user\n",
    "        print({h.name: hparams[h] for h in hparams})\n",
    "        run(\"logs/hparam_tuning/\" + run_name, hparams) #call function to train the network and log results\n",
    "\n",
    "        session_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model and training it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(50, 5, activation=\"relu\", input_shape=(28 , 28, 1)), #first layer with 50 kernel and size of 5 activation function of choice is relu\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2,2)), #second layer, pool_size(2,2) is the default but we specified it for clarity\n",
    "    tf.keras.layers.Conv2D(50, 3, activation=\"relu\"), #input_shape removed as it's only needed in the first layer and smaller kernel size to accommandate smaller images\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "    tf.keras.layers.Flatten(),  #this layer's purpose is to turn a multi-dimensional tensor into a a one-dimensional vector so we can perform classification in the next layer\n",
    "    tf.keras.layers.Dense(10) #it's impossible to produce a numerically stable loss function for all models so we won't define activation function here\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "___________________________________________________________________________\n",
      " Layer (type)                    Output Shape                  Param #     \n",
      "===========================================================================\n",
      " conv2d_10 (Conv2D)              (None, 24, 24, 50)            1300        \n",
      "                                                                           \n",
      " max_pooling2d_10 (MaxPooling2D)  (None, 12, 12, 50)           0           \n",
      "                                                                           \n",
      " conv2d_11 (Conv2D)              (None, 10, 10, 50)            22550       \n",
      "                                                                           \n",
      " max_pooling2d_11 (MaxPooling2D)  (None, 5, 5, 50)             0           \n",
      "                                                                           \n",
      " flatten_5 (Flatten)             (None, 1250)                  0           \n",
      "                                                                           \n",
      " dense_5 (Dense)                 (None, 10)                    12510       \n",
      "                                                                           \n",
      "===========================================================================\n",
      "Total params: 36,360\n",
      "Trainable params: 36,360\n",
      "Non-trainable params: 0\n",
      "___________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary(line_length=75) #this is to check if our model initialised correctly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) #from_logits = True tells tf to use softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=loss_fn, metrics=[\"accuracy\"]) # this combines our model with loss function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logging functions for Confusion matrix in order to evaluate our model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"Logs\\\\fit\\\\\" + \"run-2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codes below are provided by Tensorflow documentation on Confusion Matrix in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, class_names):\n",
    "    \"\"\"\n",
    "    Returns a matplotlib figure containing the plotted confusion matrix.\n",
    "\n",
    "    Args:\n",
    "    cm (array, shape = [n, n]): a confusion matrix of integer classes\n",
    "    class_names (array, shape = [n]): String names of the integer classes\n",
    "    \"\"\"\n",
    "    figure = plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "\n",
    "    # Normalize the confusion matrix.\n",
    "    cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "\n",
    "    # Use white text if squares are dark; otherwise black.\n",
    "    threshold = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        color = \"white\" if cm[i, j] > threshold else \"black\"\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    return figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_to_image(figure):\n",
    "    \"\"\"Converts the matplotlib plot specified by 'figure' to a PNG image and\n",
    "    returns it. The supplied figure is closed and inaccessible after this call.\"\"\"\n",
    "    \n",
    "    # Save the plot to a PNG in memory.\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    \n",
    "    # Closing the figure prevents it from being displayed directly inside the notebook.\n",
    "    plt.close(figure)\n",
    "    \n",
    "    buf.seek(0)\n",
    "    \n",
    "    # Convert PNG buffer to TF image\n",
    "    image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "    \n",
    "    # Add the batch dimension\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a file writer variable for logging purposes\n",
    "file_writer_cm = tf.summary.create_file_writer(log_dir + '/cm')\n",
    "\n",
    "def log_confusion_matrix(epoch, logs):\n",
    "    # Use the model to predict the values from the validation dataset.\n",
    "    test_pred_raw = model.predict(images_val)\n",
    "    test_pred = np.argmax(test_pred_raw, axis=1)\n",
    "\n",
    "    # Calculate the confusion matrix.\n",
    "    cm = sklearn.metrics.confusion_matrix(labels_val, test_pred)\n",
    "    \n",
    "    # Log the confusion matrix as an image summary.\n",
    "    figure = plot_confusion_matrix(cm, class_names=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'])\n",
    "    cm_image = plot_to_image(figure)\n",
    "\n",
    "    # Log the confusion matrix as an image summary.\n",
    "    with file_writer_cm.as_default():\n",
    "        tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = \"val_loss\", #monitor validation loss during training\n",
    "    mode = \"auto\",\n",
    "    min_delta = 0,\n",
    "    patience = 2,\n",
    "    verbose = 0,\n",
    "    restore_best_weights = True #this is set so that if validation loss increases for two epoch programme will stop\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log_dir = \"log\\\\fit\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch=0)\n",
    "cm_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "188/188 [==============================] - 0s 1ms/steposs: 0.2565 - accuracy\n",
      "422/422 [==============================] - 11s 24ms/step - loss: 0.2552 - accuracy: 0.9273 - val_loss: 0.0788 - val_accuracy: 0.9767\n",
      "Epoch 2/20\n",
      "188/188 [==============================] - 0s 941us/steps: 0.0695 - accuracy\n",
      "422/422 [==============================] - 11s 25ms/step - loss: 0.0692 - accuracy: 0.9792 - val_loss: 0.0482 - val_accuracy: 0.9865\n",
      "Epoch 3/20\n",
      "188/188 [==============================] - 0s 976us/steps: 0.0498 - accuracy\n",
      "422/422 [==============================] - 3s 5ms/step - loss: 0.0505 - accuracy: 0.9841 - val_loss: 0.0409 - val_accuracy: 0.9877\n",
      "Epoch 4/20\n",
      "188/188 [==============================] - 0s 920us/steps: 0.0407 - accuracy\n",
      "422/422 [==============================] - 4s 9ms/step - loss: 0.0412 - accuracy: 0.9877 - val_loss: 0.0271 - val_accuracy: 0.9920\n",
      "Epoch 5/20\n",
      "188/188 [==============================] - 0s 920us/steps: 0.0350 - accuracy\n",
      "422/422 [==============================] - 3s 5ms/step - loss: 0.0348 - accuracy: 0.9899 - val_loss: 0.0292 - val_accuracy: 0.9892\n",
      "Epoch 6/20\n",
      "188/188 [==============================] - 0s 1ms/steposs: 0.0312 - accuracy\n",
      "422/422 [==============================] - 3s 5ms/step - loss: 0.0309 - accuracy: 0.9902 - val_loss: 0.0222 - val_accuracy: 0.9932\n",
      "Epoch 7/20\n",
      "188/188 [==============================] - 0s 984us/steps: 0.0266 - accuracy\n",
      "422/422 [==============================] - 3s 5ms/step - loss: 0.0269 - accuracy: 0.9921 - val_loss: 0.0241 - val_accuracy: 0.9928\n",
      "Epoch 8/20\n",
      "188/188 [==============================] - 0s 898us/steps: 0.0251 - accuracy\n",
      "422/422 [==============================] - 3s 5ms/step - loss: 0.0249 - accuracy: 0.9923 - val_loss: 0.0214 - val_accuracy: 0.9940\n",
      "Epoch 9/20\n",
      "188/188 [==============================] - 0s 941us/steps: 0.0197 - accuracy\n",
      "422/422 [==============================] - 3s 5ms/step - loss: 0.0204 - accuracy: 0.9937 - val_loss: 0.0328 - val_accuracy: 0.9898\n",
      "Epoch 10/20\n",
      "188/188 [==============================] - 0s 941us/steps: 0.0192 - accuracy\n",
      "422/422 [==============================] - 3s 5ms/step - loss: 0.0192 - accuracy: 0.9936 - val_loss: 0.0148 - val_accuracy: 0.9957\n",
      "Epoch 11/20\n",
      "188/188 [==============================] - 0s 914us/steps: 0.0167 - accuracy\n",
      "422/422 [==============================] - 3s 5ms/step - loss: 0.0168 - accuracy: 0.9948 - val_loss: 0.0095 - val_accuracy: 0.9970\n",
      "Epoch 12/20\n",
      "188/188 [==============================] - 0s 920us/steps: 0.0146 - accuracy\n",
      "422/422 [==============================] - 3s 5ms/step - loss: 0.0146 - accuracy: 0.9956 - val_loss: 0.0108 - val_accuracy: 0.9968\n",
      "Epoch 13/20\n",
      "188/188 [==============================] - 0s 924us/steps: 0.0116 - accuracy\n",
      "422/422 [==============================] - 3s 5ms/step - loss: 0.0115 - accuracy: 0.9964 - val_loss: 0.0169 - val_accuracy: 0.9950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x230b5457b80>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_data,\n",
    "    epochs = NUM_EPOCH,\n",
    "    callbacks = [tensorboard_callback, cm_callback, early_stopping],  #it's possible to use more than one callbacks hence the []\n",
    "                                                         #note early_stopping should be the last element in the list\n",
    "                                                         #otherwise model may bug out\n",
    "    validation_data = validation_data,\n",
    "    verbose = 1 #2 means info will only be printed at end of each epoch\n",
    "                #verbose = 2 seems to introduce an error in tensorboard\n",
    "                #changed to 1 for this reason\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the training stopped early due to val_loss increase for 2 Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step - loss: 0.0289 - accuracy: 0.9904\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.0289. Test accuracy:  99.04%\n"
     ]
    }
   ],
   "source": [
    "print(\"Test loss: {0: .4f}. Test accuracy: {1: .2f}%\".format(test_loss, test_accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over 99% Accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting images and the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the test data into 2 arrays, containing the images and the corresponding labels\n",
    "for images, labels in test_data.take(1):\n",
    "    images_test = images.numpy()\n",
    "    labels_test = labels.numpy()\n",
    "\n",
    "#reshape the image into 28x28 form, suitable for matplotlib (original dimension 28x28x1)\n",
    "images_plot = np.reshape(images_test, (10000,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 2\n"
     ]
    }
   ],
   "source": [
    "# The image to be displayed and tested\n",
    "i = 1\n",
    "\n",
    "\n",
    "# Plot the image\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.axis('off')\n",
    "plt.imshow(images_plot[i-1], cmap=\"gray\", aspect='auto')\n",
    "plt.show()\n",
    "\n",
    "# Print the correct label for the image\n",
    "print(\"Label: {}\".format(labels_test[i-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 10 artists>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Obtain the model's predictions (logits)\n",
    "predictions = model.predict(images_test[i-1:i])\n",
    "\n",
    "# Convert those predictions into probabilities (recall that we incorporated the softmaxt activation into the loss function)\n",
    "probabilities = tf.nn.softmax(predictions).numpy()\n",
    "# Convert the probabilities into percentages\n",
    "probabilities = probabilities*100\n",
    "\n",
    "\n",
    "# Create a bar chart to plot the probabilities for each class\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.bar(x=[1,2,3,4,5,6,7,8,9,10], height=probabilities[0], tick_label=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above graph shows our model have almost 100% confidence in the number 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 29096), started 1 day, 0:04:25 ago. (Use '!kill 29096' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c3b80c85296f6376\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c3b80c85296f6376\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir \"logs/fit\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If tensorboard doesn't load try the following commands in cmd:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taskkill /im tensorboard.exe /f\n",
    "# del /q %TMP%\\.tensorboard-info\\*\n",
    "\n",
    "#this will end existing tensorboard process and clean any temp data associated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix and visualising it with Tensorboarb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflowgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
